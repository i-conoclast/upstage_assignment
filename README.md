
## 1. Summary

ë³¸ ë ˆí¬ì§€í† ë¦¬ì—ì„œëŠ” í•œêµ­ì–´ ë¬¸ì¥ì—ì„œ ë‘ ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ëŠ” íƒœìŠ¤í¬ë¥¼ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.  ë°ì´í„° íƒìƒ‰(EDA) ê²°ê³¼, íŠ¹íˆ no_relation í´ë˜ìŠ¤ë¡œ ì¸í•´ ì‹¬ê°í•œ ë¶ˆê· í˜• ë¬¸ì œê°€ ë°œìƒí•œë‹¤ëŠ” ì ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, íŠ¹ì • ì—”í‹°í‹° íƒ€ì… ì¡°í•©(ì˜ˆ: ORG-PER)ì—ì„œ íŠ¹ì • ê´€ê³„ê°€ ì§‘ì¤‘ì ìœ¼ë¡œ ë°œìƒí•œë‹¤ëŠ” íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆê³ , ì¼ë¶€ ì¡´ì¬í•˜ëŠ” ê¸´ ë¬¸ì¥ì— ëŒ€í•œ ì •ë³´ ì†ì‹¤ì„ ì¤„ì´ì§€ ì•Šë„ë¡ max_lengthë¥¼ ì ì ˆí•œ ìˆ˜ì¤€(128~160)ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ì— ë”°ë¼ ì—”í‹°í‹° ë§ˆì»¤(íƒ€ì… ì •ë³´ í¬í•¨ ê°€ëŠ¥), Focal Loss(gamma=1.0, alpha=0.25ë¡œ Grid Searchë¡œ ìµœì í™”), ê·¸ë¦¬ê³  linear/cosine LR ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ì„ ì ìš©í•´ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤. í‰ê°€ì—ì„œëŠ” ê³¼ì œì˜ í‰ê°€ì§€í‘œì¸ no_relationì„ ì œì™¸í•œ Micro-F1, AUPRCë¥¼ ì§€í‘œë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. í•œí¸, ì—”í‹°í‹° ìŠ¤íŒ¬ í’€ë§ê³¼ Attention ê¸°ë°˜ í’€ë§ ë“±ì„ ì¶”ê°€ ì ìš©í•´ ì—”í‹°í‹° ì§‘ì¤‘ë„ë¥¼ ë†’ì—¬ ì„±ëŠ¥ì„ ë†’ì´ê³ ì í–ˆê³ , ìŠ¤íŒ¬ ê¸°ë°˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë”ìš± ë†’ì´ê³ ì í–ˆìŠµë‹ˆë‹¤.

## 2. Experimental Results

* í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ í‰ê°€ ê²°ê³¼

| Model Variant   | Loss Function           | Label Smoothing | LR Scheduler | Entity Marker (Type included) | Span Pooling | Micro-F1 (no_relation excl.) | AUPRC   |
|-----------------|-------------------------|-----------------|-------------|--------------------------------|--------------|------------------------------|---------|
| RoBERTa base    | Focal Loss             | 0.1             | Linear      | Yes (With Type)                | Yes          | 65.4346                      | 59.9928 |
| RoBERTa large   | Focal Loss             | 0.1             | Linear      | Yes (With Type)                | Yes          | 67.3219                      | 66.0349 |
| RoBERTa large   | Focal Loss (Optimized) | 0.09            | Linear      | Yes (With Type)                | Yes          | 68.7285                      | 68.9031 |
| RoBERTa large   | Focal Loss (Optimized) | 0.09            | Cosine      | Yes (With Type)                | Yes          | 71.1004                      | 72.6258 |

## 3. Instructions

**Environment Settings**

- íŒŒì´ì¬ ë²„ì „ ê´€ë¦¬ ë„êµ¬ë¡œ `pyenv` ì‚¬ìš© (.python-version íŒŒì¼ ì°¸ê³ )
- ì˜ì¡´ì„± ê´€ë¦¬ ë„êµ¬ë¡œ `Poetry` ì‚¬ìš©
- ìƒì„¸ ë‚´ìš©ì€ Usage Instructions - 2) Environment Setup ì°¸ê³ 

**Code Structure**

```
ğŸ“¦ upstage_assignment
â”œâ”€Â data
â”‚Â Â â”œâ”€Â train.csv (ì›ë³¸)
â”‚Â Â â”œâ”€Â train_data.csv
â”‚Â Â â”œâ”€Â valid_data.csv
â”‚Â Â â””â”€Â test_data.csv
â”œâ”€Â models
â”œâ”€Â notebooks
â”‚Â Â â””â”€Â plots
â”œâ”€Â outputs
â”œâ”€Â src
â”‚Â Â â”œâ”€Â tools
â”‚Â Â â”‚Â Â â”œâ”€Â __init__.py
â”‚Â Â â”‚Â Â â”œâ”€Â dict_label_to_num.pkl
â”‚Â Â â”‚Â Â â”œâ”€Â dict_num_to_label.pkl
â”‚Â Â â”‚Â Â â””â”€Â utils.py
â”‚Â Â â”œâ”€Â config.py
â”‚Â Â â”œâ”€Â dataset.py
â”‚Â Â â”œâ”€Â inference.py
â”‚Â Â â”œâ”€Â loss.py
â”‚Â Â â”œâ”€Â metrics.py
â”‚Â Â â”œâ”€Â model.py
â”‚Â Â â””â”€Â train.py
â”œâ”€Â train.sh
â”œâ”€Â inference.sh
â”œâ”€Â README.md
â”œâ”€Â .python_version
â”œâ”€Â poetry.lock
â””â”€Â pyproject.toml
```

**Usage Instructions**

1) Data Preparation
    - data/ ë””ë ‰í† ë¦¬ì— train_data.csv, valid_data.csv, test_data.csv íŒŒì¼ ì¤€ë¹„
        - ê¸°ì¡´ train.csvë¥¼ ì‚¬ìš©í•˜ì—¬ sklearnì˜ train_test_split í•¨ìˆ˜ë¥¼ í†µí•´ train_data.csvì™€ valid_data.csv ìƒì„±
        ```
        train, valid = train_test_split(train_data, test_size=0.2, 
        stratify=train_data["label"])
        
        train.reset_index(drop=True, inplace=True)
        valid.reset_index(drop=True, inplace=True)
        
        train.to_csv("data/train_data.csv", index=False)
        valid.to_csv("data/valid_data.csv", index=False)
        ```
2) Environment Setup
    - pyenv ì„¤ì¹˜ í›„ ì§„í–‰
        ```
        pyenv install 3.10.13
        pyenv local 3.10.13
        ```

    - poetry ì„¤ì¹˜ í›„ ì§„í–‰

        ```
        poetry install # ì˜ì¡´ì„± ì„¤ì¹˜
        poetry shell # ê°€ìƒí™˜ê²½ í™œì„±í™”
        ```

    - `pyproject.toml` íŒŒì¼ ë‚´ìš©

        ```
        [tool.poetry]
        name = "upstage-assignment"
        version = "0.1.0"
        description = ""
        authors = ["sora kim <icon_o_clast@naver.com>"]
        readme = "README.md"

        [tool.poetry.dependencies]
        python = "^3.10"
        requests = "^2.32.3"
        pandas = "^2.2.3"
        tqdm = "^4.67.1"
        torch = "^2.5.1"
        transformers = "^4.47.1"
        scikit-learn = "^1.6.0"
        numpy = "^2.2.0"
        matplotlib = "^3.10.0"
        seaborn = "^0.13.2"


        [tool.poetry.group.dev.dependencies]
        ipykernel = "^6.29.5"

        [build-system]
        requires = ["poetry-core"]
        build-backend = "poetry.core.masonry.api"
        ```
    
    - (ì„ íƒ) pyenvì˜ íŒŒì´ì¬ ë²„ì „ì„ poetryê°€ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
        ```
        poetry env use 3.10.13
        ```


3) Model Training
    ```
    bash train.sh
    ```

    OR

    ```
    python src/train.py \
    --train_file data/train_data.csv \
    --valid_file data/valid_data.csv \
    --model_name_or_path klue/roberta-large \
    --max_length 128 \
    --batch_size 16 \
    --num_epochs 10 \
    --learning_rate 3e-5 \
    --use_cuda True \
    --use_entity_markers True \
    --use_entity_types True \
    --use_span_pooling True \
    --dropout 0.1 \
    --label2id_path utils/dict_label_to_num.pkl \
    --id2label_path utils/dict_num_to_label.pkl \
    --model_dir models \
    --focal_loss \
    --label_smoothing 0.09 \
    --alpha 1.0 \
    --gamma 2.0 \
    ```

    - ì„¸ë¶€ê°’ì€ ì¡°ì • ê°€ëŠ¥
    - í•™ìŠµ ê²°ê³¼ëŠ” models/ ë””ë ‰í† ë¦¬ì— ì €ì¥

4) Model Inference
    ```
    bash inference.sh
    ```

    OR

    ```
    python src/inference.py \
    --model_file models/{ëª¨ë¸ëª…}.pth \
    --model_dir models \
    --output_dir outputs \
    --test_file data/test_data.csv \
    ```

    - ì¶”ë¡  ê²°ê³¼ëŠ” csv íŒŒì¼ë¡œ outputs/ ë””ë ‰í† ë¦¬ì— ì €ì¥
        - id, pred_label, probs ì»¬ëŸ¼ í¬í•¨
        - idëŠ” ì›ë³¸ ë°ì´í„°ì˜ idì™€ ë™ì¼
        - pred_labelì€ ì˜ˆì¸¡ ë ˆì´ë¸”
        - probsëŠ” ì˜ˆì¸¡ í™•ë¥ 

## 4. Approach

**Exploratory Data Analysis(EDA)**
- ë¼ë²¨ ë¶„í¬: no_relation í´ë˜ìŠ¤ê°€ ìš°ì„¸í•˜ê³ , ë‹¤ìˆ˜ì˜ í¬ì†Œ í´ë˜ìŠ¤ê°€ ì¡´ì¬í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ ì‰½ê²Œ no_relationìœ¼ë¡œë§Œ ì˜ˆì¸¡í•˜ëŠ” í¸í–¥ì„ ë§‰ê¸° ìœ„í•´ ë¶ˆê· í˜•ì„ í•´ì†Œí•˜ëŠ” ì „ëµì´ í•„ìš”í•œ ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

- ë¬¸ì¥ ê¸¸ì´ & ì—”í‹°í‹° íƒ€ì…: í‰ê·  100ìë‚´ì™¸, ìµœì¥ 400ì ì´ìƒì¸ ê²½ìš°ë„ ìˆì–´ max_length 128-160 ì •ë„ê°€ ì ì ˆí•˜ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤. ORG-PER ë“± íŠ¹ì • íƒ€ì… ì¡°í•©ì—ì„œ íŠ¹ì • ê´€ê³„(org:top_members/employees)ê°€ ì§‘ì¤‘ë˜ëŠ” íŒ¨í„´ì´ ë“œëŸ¬ë‚˜, íƒ€ì… ì •ë³´ë¥¼ í™œìš©í•˜ë©´ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—”í‹°í‹° íƒ€ì… ì •ë³´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ì— ì•Œë ¤ì£¼ë©´ (E1-ORG, E2-PER ë“±) ê´€ê³„ ì¶”ì¶œ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒì´ë¼ëŠ” ê°€ì •ì„ ì„¸ì› ìŠµë‹ˆë‹¤.

**Model & Architecture**
- Backbone Model
    - KoELECTRA, KLUE-RoBERTa, KoBERT ë“± í•œêµ­ì–´ íŠ¹í™” ì‚¬ì „í•™ìŠµëª¨ë¸(PLM) ì¤‘ KLUE-RoBERTaë¥¼ ì£¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
    - ë¬¸ì¥ ì…ë ¥ í›„ ë§ˆì§€ë§‰ hidden stateë¥¼ í™œìš©í•˜ë˜, ë‹¨ìˆœ [CLS] ì„ë² ë”© ëŒ€ì‹  ì—”í‹°í‹° ìŠ¤íŒ¬ ì„ë² ë”©ì´ë‚˜ Attention Poolingì„ ì‚¬ìš©í•˜ë„ë¡ ê°œì„ í•˜ì˜€ìŠµë‹ˆë‹¤.

- Entity Marker
    - EDA ê²°ê³¼, ì—”í‹°í‹° íƒ€ì…ì´ íŠ¹ì • ê´€ê³„ì™€ ë°€ì ‘í•˜ë¯€ë¡œ ë§ˆì»¤ì— íƒ€ì…ê¹Œì§€ ëª…ì‹œí•˜ì˜€ìŠµë‹ˆë‹¤.([E1-ORG], [E2-PER] ë“±)
    - ëª¨ë¸ ì¸ì½”ë”© ì‹œ, ì´ ë§ˆì»¤ ì •ë³´ë¥¼ ë³„ë„ì˜ ìŠ¤í˜ì…œ í† í°ìœ¼ë¡œ ì²˜ë¦¬í•´ ì—”í‹°í‹° ì£¼ë³€ í† í°ì— Attentionì´ ì§‘ì¤‘ë˜ë„ë¡ ìœ ë„í–ˆìŠµë‹ˆë‹¤.

- Span Pooling / Attention Pooling
    - Span Pooling: ì—”í‹°í‹° êµ¬ê°„(ì˜ˆ: [E1] ~ [/E1]) ì„ë² ë”©ì„ í‰ê·  ë˜ëŠ” max poolingìœ¼ë¡œ ëª¨ìœ¼ê³ , ì´ë¥¼ ìµœì¢… ê´€ê³„ ë¶„ë¥˜ì— í™œìš©í–ˆìŠµë‹ˆë‹¤.
    - Attention Pooling: ì—”í‹°í‹° ìŠ¤íŒ¬ ë‚´ í† í°ë§ˆë‹¤ learnable weight(Attention)ë¥¼ ì ìš©í•´ ì¤‘ìš”í•œ í† í°ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬, ì—”í‹°í‹° ì •ë³´ë¥¼ ë”ìš± ì •í™•íˆ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.


**Training/Evaluation Scheme**

- Loss Function & ë¶ˆê· í˜• ëŒ€ì‘
    - Focal Loss ì‚¬ìš©: $\gamma=1.0$ì™€ $\alpha=0.25$ë¥¼ Grid Searchë¡œ ìµœì í™”í•˜ì—¬ í¬ì†Œ í´ë˜ìŠ¤ì— ëŒ€í•œ í•™ìŠµì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤.
    - ê¸°ì¡´ Cross Entropy ëŒ€ë¹„ í¬ì†Œ í´ë˜ìŠ¤ ì‹ë³„ë ¥ì´ í–¥ìƒë¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
    
- Learning Rate Scheduler
    - Linearì™€ Cosine ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë‘ ì‹¤í—˜í•˜ì—¬, ì´ˆë°˜ í•™ìŠµ ì•ˆì •ì„±ê³¼ í›„ë°˜ ìˆ˜ë ´ íŠ¹ì„±ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤.
    - Cosine ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ìµœì¢…ì ìœ¼ë¡œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
    
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    - max_length: 128~160 ë²”ìœ„ ì‹œí—˜ í›„, ê¸´ ë¬¸ì¥ì˜ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ë° 160 ì „í›„ê°€ ì ì ˆí•œ ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

- í‰ê°€ ì§€í‘œ
    - no_relation ì œì™¸ Micro-F1: í¬ì†Œ í´ë˜ìŠ¤ ì¸ì‹ë ¥ì„ ì¤‘ì‹œí•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.    
    - AUPRC: ëª¨ë“  í´ë˜ìŠ¤(í¬í•¨ no_relation)ì— ëŒ€í•œ ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„ ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

- ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤
    - Baseline(RoBERTa+CE) â†’ Focal Loss ì¶”ê°€ â†’ ì—”í‹°í‹° ë§ˆì»¤/íƒ€ì… í‘œê¸° â†’ ìŠ¤íŒ¬ í’€ë§/Attention ê¸°ë°˜ pooling â†’ ìŠ¤ì¼€ì¤„ëŸ¬ ë³€ê²½(Linearâ†’Cosine) ë“± ë‹¨ê³„ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤.
    - validation setì—ì„œ Micro-F1, AUPRC ì¸¡ì • í›„ best ëª¨ë¸ì„ ì„ ì •í–ˆìŠµë‹ˆë‹¤.

**Literature & Relevant Works**

1.	KLUE Benchmark ë° RE íƒœìŠ¤í¬([KLUE-RE](https://github.com/KLUE-benchmark/KLUE/tree/main))
    - í•œêµ­ì–´ ì „ìš© ë²¤ì¹˜ë§ˆí¬ë¡œ, KLUE-REì—ì„œ ë‹¤ì–‘í•œ baseline ë° SOTA ëª¨ë¸ì´ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.
    - ë¬¸í—Œì—ì„œ ì—”í‹°í‹° ë§ˆì»¤, íƒ€ì… ì •ë³´ í™œìš© ì‹œ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒëœë‹¤ê³  ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤.

2.	Span-based Models (LUKE([LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057)), SpanBERT([SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/abs/1907.10529)))
    - LUKE: ì—”í‹°í‹°-aware pretrainingìœ¼ë¡œ ê´€ê³„ ì¶”ì¶œì— ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
    - SpanBERT: ìŠ¤íŒ¬ ë‹¨ìœ„ ë§ˆìŠ¤í‚¹ ë° í•™ìŠµ ê¸°ë²•ì´ ê´€ê³„ íŒë‹¨ì— ë„ì›€ì´ ëœë‹¤ëŠ” ì ì„ ì‹œì‚¬í–ˆìŠµë‹ˆë‹¤.

3.	Focal Loss([Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002))
    - Tsung-Yi Lin (2017)ì—ì„œ ì œì•ˆëœ ê¸°ë²•ìœ¼ë¡œ, \gamma, \alpha ì¡°ì • ì‹œ ë¶ˆê· í˜• ë°ì´í„°ì—ì„œì˜ í¬ì†Œ í´ë˜ìŠ¤ ì‹ë³„ë ¥ì´ ê°œì„ ëœ ê²ƒì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.

4.	Attention Pooling ë° Entity Marker ì—°êµ¬([An Improved Baseline for Sentence-level Relation Extraction](https://arxiv.org/abs/2102.01373))
    - ê¸°ì¡´ NER, RE ì—°êµ¬ì—ì„œ [E1], [E2] ë§ˆì»¤ë¥¼ ì‚½ì… ì‹œ ëª¨ë¸ì´ ì—”í‹°í‹° ìœ„ì¹˜ì™€ íƒ€ì…ì„ ë”ìš± ëª…í™•íˆ ì¸ì§€í•´ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì¸ë‹¤ê³  ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤.
    - Attention Poolingì€ ë§ì€ NLP ê³¼ì œì—ì„œ ë¬¸ë§¥ ì¤‘ìš” í† í°ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„ ì— ê¸°ì—¬í•©ë‹ˆë‹¤.

**Future Work**

1.	ê³ ê¸‰ ë°ì´í„° ì¦ê°•
    - Back-translation ì™¸ì—ë„ paraphrasing, synonym replacement, style transfer ë“±ì„ ê²°í•©í•´ ë°ì´í„° ë‹¤ë³€í™”ë¥¼ ë„ëª¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - í¬ì†Œ í´ë˜ìŠ¤ ìƒ˜í”Œì„ ì¤‘ì‹¬ìœ¼ë¡œ ì¦ê°•í•˜ë©´ ë¶ˆê· í˜• ì™„í™” íš¨ê³¼ê°€ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2.	ë” í° ëª¨ë¸ ë° ì•™ìƒë¸”
    - KoELECTRAÂ·KLUE-RoBERTaÂ·LUKE ë“± ëª¨ë¸ ì•™ìƒë¸”ì„ ì‹œë„í•´ ì¶”ê°€ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ëª¨ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
